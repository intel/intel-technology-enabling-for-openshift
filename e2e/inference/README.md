## Intel AI Inference E2E Solution for OpenShift

### Overview
Intel AI inference e2e solution is built upon provisioning Intel® Data Center GPU Flex Series on Intel® Xeon® processors for OpenShift. The two following AI inference modes are used to test it:
* **Interactive Mode**
[Red Hat OpenShift Data Science (RHODS)](https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-data-science) and [Open Data Hub (ODH)](https://github.com/opendatahub-io) provide Intel® OpenVINO™ based [Jupyter Notebook](https://jupyter.org/) to help users interactively debug the inference applications or optimize the models with Intel Data Center GPU cards on OCP.
*	**Deployment Mode**
[Intel OpenVINO™ Toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html) and [Operator](https://github.com/openvinotoolkit/operator) provide the [OpenVINO Model Server (OVMS)](https://github.com/openvinotoolkit/model_server) for users to deploy and use their inference workloads with Intel Data Center GPU cards on OCP in cloud or edge environment.

     `Note: This mode's verification is ongoing`

### Deploy Intel AI Inference E2E Solution

* **Install RHODS on OpenShift**
* **Install Intel OpenVINO Toolkit Operator**

### Run Interactive Mode Demo

### Run Deployment Mode Demo 